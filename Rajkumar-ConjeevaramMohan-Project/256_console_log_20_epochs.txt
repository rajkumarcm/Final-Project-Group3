/usr/bin/python3 /home/ubuntu/.pycharm_helpers/pydev/pydevd.py --multiprocess --qt-support=auto --client localhost --port 45803 --file /home/ubuntu/Text-Summarization/NLP_project_256.py 
Connected to pydev debugger (build 223.8617.48)
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (5.1.0) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
2023-05-06 16:05:52.370169: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-05-06 16:05:54.421297: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2023-05-06 16:05:54.421392: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2023-05-06 16:05:54.421403: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.

load data...

data frames created...

data sets created...
Using cuda_amp half precision backend

model and training sequence created...

begin training...
/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 500
  Num Epochs = 20
  Instantaneous batch size per device = 30
  Total train batch size (w. parallel, distributed & accumulation) = 30
  Gradient Accumulation steps = 1
  Total optimization steps = 320
  Number of trainable parameters = 60506624
  0%|          | 0/320 [00:00<?, ?it/s]The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  5%|â–Œ         | 16/320 [00:21<05:46,  1.14s/it]***** Running Evaluation *****
  Num examples = 250
  Batch size = 30
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

  0%|          | 0/8 [00:00<?, ?it/s]
 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:02,  2.54it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:01<00:02,  1.79it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:02<00:02,  1.55it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:03<00:02,  1.43it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:03<00:01,  1.37it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:04<00:00,  1.33it/s]
                                                
  5%|â–Œ         | 16/320 [00:31<05:46,  1.14s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:08<00:00,  1.31it/s]
                                             {'eval_loss': 1.4215182065963745, 'eval_rouge1': 0.006, 'eval_rouge2': 0.0024, 'eval_rougeL': 0.0049, 'eval_rougeLsum': 0.005, 'eval_gen_len': 0.475, 'eval_runtime': 9.7214, 'eval_samples_per_second': 25.716, 'eval_steps_per_second': 0.926, 'epoch': 1.0}
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 10%|â–ˆ         | 32/320 [00:51<05:33,  1.16s/it]***** Running Evaluation *****
  Num examples = 250
  Batch size = 30
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

  0%|          | 0/8 [00:00<?, ?it/s]
 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:02,  2.53it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:01<00:02,  1.79it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:02<00:02,  1.55it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:03<00:02,  1.44it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:03<00:01,  1.38it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:04<00:00,  1.34it/s]
                                                
{'eval_loss': 1.3914436101913452, 'eval_rouge1': 0.2268, 'eval_rouge2': 0.1106, 'eval_rougeL': 0.1934, 'eval_rougeLsum': 0.1935, 'eval_gen_len': 17.7333, 'eval_runtime': 10.1646, 'eval_samples_per_second': 24.595, 'eval_steps_per_second': 0.885, 'epoch': 2.0}
 10%|â–ˆ         | 32/320 [01:01<05:33,  1.16s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:08<00:00,  1.32it/s]
                                             The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 15%|â–ˆâ–Œ        | 48/320 [01:20<05:14,  1.16s/it]***** Running Evaluation *****
  Num examples = 250
  Batch size = 30
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

  0%|          | 0/8 [00:00<?, ?it/s]
 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:02,  2.53it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:01<00:02,  1.78it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:02<00:02,  1.54it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:03<00:02,  1.43it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:03<00:01,  1.37it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:04<00:00,  1.33it/s]
                                                
 15%|â–ˆâ–Œ        | 48/320 [01:31<05:14,  1.16s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:08<00:00,  1.31it/s]
                                             {'eval_loss': 1.4020613431930542, 'eval_rouge1': 0.2447, 'eval_rouge2': 0.1191, 'eval_rougeL': 0.2034, 'eval_rougeLsum': 0.2038, 'eval_gen_len': 19.0, 'eval_runtime': 10.2726, 'eval_samples_per_second': 24.337, 'eval_steps_per_second': 0.876, 'epoch': 3.0}
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 20%|â–ˆâ–ˆ        | 64/320 [01:50<04:57,  1.16s/it]***** Running Evaluation *****
  Num examples = 250
  Batch size = 30
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

  0%|          | 0/8 [00:00<?, ?it/s]
 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:02,  2.53it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:01<00:02,  1.78it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:02<00:02,  1.55it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:03<00:02,  1.44it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:03<00:01,  1.38it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:04<00:00,  1.34it/s]
                                                
{'eval_loss': 1.4248136281967163, 'eval_rouge1': 0.2451, 'eval_rouge2': 0.1183, 'eval_rougeL': 0.2048, 'eval_rougeLsum': 0.2047, 'eval_gen_len': 19.0, 'eval_runtime': 10.2414, 'eval_samples_per_second': 24.411, 'eval_steps_per_second': 0.879, 'epoch': 4.0}
 20%|â–ˆâ–ˆ        | 64/320 [02:01<04:57,  1.16s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:08<00:00,  1.31it/s]
                                             The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 25%|â–ˆâ–ˆâ–Œ       | 80/320 [02:20<04:38,  1.16s/it]***** Running Evaluation *****
  Num examples = 250
  Batch size = 30
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

  0%|          | 0/8 [00:00<?, ?it/s]
 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:02,  2.52it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:01<00:02,  1.79it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:02<00:02,  1.55it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:03<00:02,  1.43it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:03<00:01,  1.37it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:04<00:00,  1.34it/s]
                                                
{'eval_loss': 1.454579472541809, 'eval_rouge1': 0.2491, 'eval_rouge2': 0.1172, 'eval_rougeL': 0.2043, 'eval_rougeLsum': 0.2044, 'eval_gen_len': 19.0, 'eval_runtime': 10.2506, 'eval_samples_per_second': 24.389, 'eval_steps_per_second': 0.878, 'epoch': 5.0}
 25%|â–ˆâ–ˆâ–Œ       | 80/320 [02:31<04:38,  1.16s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:08<00:00,  1.31it/s]
                                             The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 30%|â–ˆâ–ˆâ–ˆ       | 96/320 [02:50<04:19,  1.16s/it]***** Running Evaluation *****
  Num examples = 250
  Batch size = 30
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

  0%|          | 0/8 [00:00<?, ?it/s]
 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:02,  2.52it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:01<00:02,  1.78it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:02<00:02,  1.53it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:03<00:02,  1.43it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:03<00:01,  1.36it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:04<00:00,  1.32it/s]
                                                
{'eval_loss': 1.4907978773117065, 'eval_rouge1': 0.2498, 'eval_rouge2': 0.1167, 'eval_rougeL': 0.2082, 'eval_rougeLsum': 0.2078, 'eval_gen_len': 19.0, 'eval_runtime': 10.3235, 'eval_samples_per_second': 24.217, 'eval_steps_per_second': 0.872, 'epoch': 6.0}
 30%|â–ˆâ–ˆâ–ˆ       | 96/320 [03:01<04:19,  1.16s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:08<00:00,  1.30it/s]
                                             The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 112/320 [03:20<04:02,  1.16s/it]***** Running Evaluation *****
  Num examples = 250
  Batch size = 30
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

  0%|          | 0/8 [00:00<?, ?it/s]
 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:02,  2.51it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:01<00:02,  1.77it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:02<00:02,  1.54it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:03<00:02,  1.43it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:03<00:01,  1.36it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:04<00:00,  1.33it/s]
                                                 
{'eval_loss': 1.532562255859375, 'eval_rouge1': 0.2426, 'eval_rouge2': 0.1131, 'eval_rougeL': 0.2007, 'eval_rougeLsum': 0.2007, 'eval_gen_len': 19.0, 'eval_runtime': 10.3109, 'eval_samples_per_second': 24.246, 'eval_steps_per_second': 0.873, 'epoch': 7.0}
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 112/320 [03:31<04:02,  1.16s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:08<00:00,  1.31it/s]
                                             The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 128/320 [03:50<03:42,  1.16s/it]***** Running Evaluation *****
  Num examples = 250
  Batch size = 30
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

  0%|          | 0/8 [00:00<?, ?it/s]
 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:02,  2.52it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:01<00:02,  1.78it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:02<00:02,  1.54it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:03<00:02,  1.42it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:03<00:01,  1.36it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:04<00:00,  1.32it/s]
                                                 
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 128/320 [04:01<03:42,  1.16s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:08<00:00,  1.30it/s]
                                             {'eval_loss': 1.5589772462844849, 'eval_rouge1': 0.2494, 'eval_rouge2': 0.1201, 'eval_rougeL': 0.2058, 'eval_rougeLsum': 0.2056, 'eval_gen_len': 19.0, 'eval_runtime': 10.3861, 'eval_samples_per_second': 24.071, 'eval_steps_per_second': 0.867, 'epoch': 8.0}
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 144/320 [04:20<03:24,  1.16s/it]***** Running Evaluation *****
  Num examples = 250
  Batch size = 30
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

  0%|          | 0/8 [00:00<?, ?it/s]
 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:02,  2.48it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:01<00:02,  1.75it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:02<00:02,  1.51it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:03<00:02,  1.40it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:04<00:01,  1.35it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:04<00:00,  1.31it/s]
                                                 
{'eval_loss': 1.5936521291732788, 'eval_rouge1': 0.2487, 'eval_rouge2': 0.12, 'eval_rougeL': 0.206, 'eval_rougeLsum': 0.2058, 'eval_gen_len': 19.0, 'eval_runtime': 10.4192, 'eval_samples_per_second': 23.994, 'eval_steps_per_second': 0.864, 'epoch': 9.0}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 144/320 [04:31<03:24,  1.16s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:08<00:00,  1.29it/s]
                                             The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 160/320 [04:51<03:06,  1.17s/it]***** Running Evaluation *****
  Num examples = 250
  Batch size = 30
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

  0%|          | 0/8 [00:00<?, ?it/s]
 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:02,  2.48it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:01<00:02,  1.74it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:02<00:02,  1.52it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:03<00:02,  1.41it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:04<00:01,  1.35it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:04<00:00,  1.32it/s]
                                                 
{'eval_loss': 1.6206157207489014, 'eval_rouge1': 0.2442, 'eval_rouge2': 0.1149, 'eval_rougeL': 0.202, 'eval_rougeLsum': 0.2014, 'eval_gen_len': 19.0, 'eval_runtime': 10.3719, 'eval_samples_per_second': 24.104, 'eval_steps_per_second': 0.868, 'epoch': 10.0}
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 160/320 [05:02<03:06,  1.17s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:08<00:00,  1.29it/s]
                                             The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 176/320 [05:21<02:47,  1.16s/it]***** Running Evaluation *****
  Num examples = 250
  Batch size = 30
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

  0%|          | 0/8 [00:00<?, ?it/s]
 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:02,  2.50it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:01<00:02,  1.77it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:02<00:02,  1.53it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:03<00:02,  1.42it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:03<00:01,  1.36it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:04<00:00,  1.32it/s]
                                                 
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 176/320 [05:32<02:47,  1.16s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:08<00:00,  1.30it/s]
                                             {'eval_loss': 1.6506613492965698, 'eval_rouge1': 0.2416, 'eval_rouge2': 0.1129, 'eval_rougeL': 0.1996, 'eval_rougeLsum': 0.1991, 'eval_gen_len': 19.0, 'eval_runtime': 10.3205, 'eval_samples_per_second': 24.224, 'eval_steps_per_second': 0.872, 'epoch': 11.0}
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 192/320 [05:51<02:28,  1.16s/it]***** Running Evaluation *****
  Num examples = 250
  Batch size = 30
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

  0%|          | 0/8 [00:00<?, ?it/s]
 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:02,  2.53it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:01<00:02,  1.79it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:02<00:02,  1.54it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:03<00:02,  1.43it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:03<00:01,  1.37it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:04<00:00,  1.33it/s]
                                                 
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 192/320 [06:02<02:28,  1.16s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:09<00:00,  1.31it/s]
                                             {'eval_loss': 1.678846001625061, 'eval_rouge1': 0.2403, 'eval_rouge2': 0.1108, 'eval_rougeL': 0.1978, 'eval_rougeLsum': 0.1975, 'eval_gen_len': 19.0, 'eval_runtime': 10.5624, 'eval_samples_per_second': 23.669, 'eval_steps_per_second': 0.852, 'epoch': 12.0}
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 208/320 [06:21<02:09,  1.16s/it]***** Running Evaluation *****
  Num examples = 250
  Batch size = 30
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

  0%|          | 0/8 [00:00<?, ?it/s]
 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:02,  2.50it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:01<00:02,  1.76it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:02<00:02,  1.53it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:03<00:02,  1.42it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:04<00:01,  1.36it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:04<00:00,  1.32it/s]
                                                 
{'eval_loss': 1.713276743888855, 'eval_rouge1': 0.2434, 'eval_rouge2': 0.114, 'eval_rougeL': 0.201, 'eval_rougeLsum': 0.2008, 'eval_gen_len': 19.0, 'eval_runtime': 10.7106, 'eval_samples_per_second': 23.341, 'eval_steps_per_second': 0.84, 'epoch': 13.0}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 208/320 [06:32<02:09,  1.16s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:09<00:00,  1.29it/s]
                                             The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 224/320 [06:52<01:51,  1.16s/it]***** Running Evaluation *****
  Num examples = 250
  Batch size = 30
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

  0%|          | 0/8 [00:00<?, ?it/s]
 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:02,  2.51it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:01<00:02,  1.77it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:02<00:02,  1.53it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:03<00:02,  1.43it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:03<00:01,  1.37it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:04<00:00,  1.33it/s]
                                                 
{'eval_loss': 1.7227782011032104, 'eval_rouge1': 0.2431, 'eval_rouge2': 0.1133, 'eval_rougeL': 0.2, 'eval_rougeLsum': 0.1998, 'eval_gen_len': 19.0, 'eval_runtime': 10.2892, 'eval_samples_per_second': 24.297, 'eval_steps_per_second': 0.875, 'epoch': 14.0}
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 224/320 [07:02<01:51,  1.16s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:08<00:00,  1.30it/s]
                                             The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 240/320 [07:21<01:32,  1.16s/it]***** Running Evaluation *****
  Num examples = 250
  Batch size = 30
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

  0%|          | 0/8 [00:00<?, ?it/s]
 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:02,  2.52it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:01<00:02,  1.77it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:02<00:02,  1.54it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:03<00:02,  1.43it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:03<00:01,  1.37it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:04<00:00,  1.33it/s]
                                                 
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 240/320 [07:32<01:32,  1.16s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:08<00:00,  1.31it/s]
                                             {'eval_loss': 1.7436960935592651, 'eval_rouge1': 0.242, 'eval_rouge2': 0.1133, 'eval_rougeL': 0.1995, 'eval_rougeLsum': 0.1993, 'eval_gen_len': 19.0, 'eval_runtime': 10.2753, 'eval_samples_per_second': 24.33, 'eval_steps_per_second': 0.876, 'epoch': 15.0}
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 256/320 [07:52<01:14,  1.16s/it]***** Running Evaluation *****
  Num examples = 250
  Batch size = 30
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

  0%|          | 0/8 [00:00<?, ?it/s]
 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:02,  2.54it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:01<00:02,  1.78it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:02<00:02,  1.55it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:03<00:02,  1.43it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:03<00:01,  1.37it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:04<00:00,  1.33it/s]
                                                 
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 256/320 [08:02<01:14,  1.16s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:08<00:00,  1.31it/s]
                                             {'eval_loss': 1.761875867843628, 'eval_rouge1': 0.2406, 'eval_rouge2': 0.1124, 'eval_rougeL': 0.1962, 'eval_rougeLsum': 0.1956, 'eval_gen_len': 19.0, 'eval_runtime': 10.3008, 'eval_samples_per_second': 24.27, 'eval_steps_per_second': 0.874, 'epoch': 16.0}
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 272/320 [08:22<00:55,  1.17s/it]***** Running Evaluation *****
  Num examples = 250
  Batch size = 30
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

  0%|          | 0/8 [00:00<?, ?it/s]
 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:02,  2.53it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:01<00:02,  1.78it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:02<00:02,  1.55it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:03<00:02,  1.43it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:03<00:01,  1.37it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:04<00:00,  1.33it/s]
                                                 
{'eval_loss': 1.7783961296081543, 'eval_rouge1': 0.239, 'eval_rouge2': 0.1115, 'eval_rougeL': 0.1965, 'eval_rougeLsum': 0.1961, 'eval_gen_len': 19.0, 'eval_runtime': 10.2836, 'eval_samples_per_second': 24.311, 'eval_steps_per_second': 0.875, 'epoch': 17.0}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 272/320 [08:32<00:55,  1.17s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:08<00:00,  1.31it/s]
                                             The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 288/320 [08:52<00:37,  1.16s/it]***** Running Evaluation *****
  Num examples = 250
  Batch size = 30
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

  0%|          | 0/8 [00:00<?, ?it/s]
 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:02,  2.52it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:01<00:02,  1.78it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:02<00:02,  1.54it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:03<00:02,  1.42it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:03<00:01,  1.36it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:04<00:00,  1.33it/s]
                                                 
{'eval_loss': 1.7933883666992188, 'eval_rouge1': 0.2409, 'eval_rouge2': 0.113, 'eval_rougeL': 0.1979, 'eval_rougeLsum': 0.1973, 'eval_gen_len': 19.0, 'eval_runtime': 10.2748, 'eval_samples_per_second': 24.331, 'eval_steps_per_second': 0.876, 'epoch': 18.0}
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 288/320 [09:03<00:37,  1.16s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:08<00:00,  1.31it/s]
                                             The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 304/320 [09:22<00:18,  1.16s/it]***** Running Evaluation *****
  Num examples = 250
  Batch size = 30
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

  0%|          | 0/8 [00:00<?, ?it/s]
 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:02,  2.48it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:01<00:02,  1.75it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:02<00:02,  1.52it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:03<00:02,  1.41it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:04<00:01,  1.35it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:04<00:00,  1.31it/s]
                                                 
{'eval_loss': 1.7972394227981567, 'eval_rouge1': 0.2422, 'eval_rouge2': 0.1143, 'eval_rougeL': 0.1997, 'eval_rougeLsum': 0.1991, 'eval_gen_len': 19.0, 'eval_runtime': 10.3962, 'eval_samples_per_second': 24.047, 'eval_steps_per_second': 0.866, 'epoch': 19.0}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 304/320 [09:33<00:18,  1.16s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:08<00:00,  1.29it/s]
                                             The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 320/320 [09:52<00:00,  1.16s/it]***** Running Evaluation *****
  Num examples = 250
  Batch size = 30
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: input_attention_mask, output_attention_mask. If input_attention_mask, output_attention_mask are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.
You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

  0%|          | 0/8 [00:00<?, ?it/s]
 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:02,  2.52it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:01<00:02,  1.78it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:02<00:02,  1.54it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:03<00:02,  1.43it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:03<00:01,  1.37it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:04<00:00,  1.33it/s]
                                                 
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 320/320 [10:03<00:00,  1.16s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:08<00:00,  1.31it/s]
                                             

Training completed. Do not forget to share your model on huggingface.co/models =)


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 320/320 [10:03<00:00,  1.89s/it]
{'eval_loss': 1.8007614612579346, 'eval_rouge1': 0.2413, 'eval_rouge2': 0.1141, 'eval_rougeL': 0.1987, 'eval_rougeLsum': 0.1979, 'eval_gen_len': 19.0, 'eval_runtime': 10.3439, 'eval_samples_per_second': 24.169, 'eval_steps_per_second': 0.87, 'epoch': 20.0}
{'train_runtime': 603.3999, 'train_samples_per_second': 16.573, 'train_steps_per_second': 0.53, 'train_loss': 0.9164646148681641, 'epoch': 20.0}

begin testing...

0/250 summarizations completed...
10/250 summarizations completed...
20/250 summarizations completed...
30/250 summarizations completed...
40/250 summarizations completed...
50/250 summarizations completed...
60/250 summarizations completed...
70/250 summarizations completed...
80/250 summarizations completed...
90/250 summarizations completed...
100/250 summarizations completed...
110/250 summarizations completed...
120/250 summarizations completed...
130/250 summarizations completed...
140/250 summarizations completed...
150/250 summarizations completed...
160/250 summarizations completed...
170/250 summarizations completed...
180/250 summarizations completed...
190/250 summarizations completed...
200/250 summarizations completed...
210/250 summarizations completed...
220/250 summarizations completed...
230/250 summarizations completed...
240/250 summarizations completed...

Example of First 10 Summaries Produced Compared to Original Highlights from Data

0:
	Original Summary: Experts question if packed out planes are putting passengers at risk. U.S consumer advisory group says minimum space must be stipulated. Safety tests conducted on planes with more leg room than airlines offer.
	Produced Summary: Increasing numbers of people taking to the skies are seeing shrinking space on planes. Experts say that while the government is happy to set standards for animals flying on planes, it doesn't stipulate a minimum amount of space for humans. Tests conducted by the FAA use planes with a flier and a flier to fight for space in the overhead lockers, crashing elbows and seat back kicking.
1:
	Original Summary: Drunk teenage boy climbed into lion enclosure at zoo in west India. Rahul Kumar, 17, ran towards animals shouting 'Today I kill a lion!' Fortunately he fell into a moat before reaching lions and was rescued.
	Produced Summary: Rahul Kumar, 17, climbed into lions' enclosure at zoo in western India. He had been sitting near the enclosure fence and began running towards them. He explained afterwards that he was drunk and 'thought I'd stand a good chance' against the animals.
2:
	Original Summary: Nottingham Forest are close to extending Dougie Freedman's contract. The Forest boss took over from former manager Stuart Pearce in February. Freedman has since lead the club to ninth in the Championship.
	Produced Summary: Dougie Freedman set to sign new deal at Nottingham Forest on two-year deal. Freedman has stabilised Forest since he replaced Stuart Pearce in February. Forest have made an audacious attempt on the play-off places when he replaced Stuart Pearce.
3:
	Original Summary: Fiorentina goalkeeper Neto has been linked with Liverpool and Arsenal. Neto joined Firoentina from Brazilian outfit Atletico Paranaense in 2011. He is also wanted by PSG and Spanish clubs, according to his agent. CLICK HERE for the latest Liverpool news.
	Produced Summary: Stefano Castagna says Neto is wanted by a number of top European clubs. The Brazilian goalkeeper was linked with a move to Fiorentina by Liverpool in January. A move for the 25-year-old happened at the end of the season. But his agent has revealed no decision about his future has been made.
4:
	Original Summary: Tell-all interview with the reality TV star, 69, will air on Friday April 24. It comes amid continuing speculation about his transition to a woman and following his involvement in a deadly car crash in February. The interview will also be one of Diane Sawyer's first appearances on television following the sudden death of her husband last year.
	Produced Summary: Jenner will speak out in a 'far-ranging' interview with Diane Sawyer later this month. The former Olympian and reality TV star, 65, will speak on April 24. The interview comes amid growing speculation about the father-of-six's transition to a woman, and follows closely behind his involvement in a deadly car crash in California in February.
5:
	Original Summary: Giant pig fell into the swimming pool at his home in Ringwood, Hampshire. It took the efforts of a team of firefighters to winch him out of the water. A wayward horse also had to be rescued from a swimming pool in Sussex.
	Produced Summary: Dorset Fire and Rescue team had to use slide boards and strops to haul the huge pig out of pool. The prize porker had fallen into the pool in an upmarket neighbourhood in Hampshire. His owners had been taking him for a walk around the garden when the animal plunged into the water. The crew had to use the pig's strops and strops to help haul the pig out of the pool.
6:
	Original Summary: Figures show that while millions still tune in they listen for shorter bursts. Average listener spent ten hours a week tuning in last three months of 2014. This was 14% down on decade earlier, when people tuned in for 11.6 hours. The BBC Trust has cleared the way for firms to buy their way into lifestyle programmes on the World News channel in a product placement experiment. For example, publishers could pay to have their books reviewed on Talking Books. The BBC Trust will review the scheme in a year.
	Produced Summary: The length of time people spend listening to BBC radio dropped to its lowest ever level. The average listener spent just ten hours a week tuning in to the company in 2014. This was 14 per cent down on a decade earlier, when listeners clocked up an average of 11.6 hours a week. Sources blamed a change in habits amongst young people.
7:
	Original Summary: Show will return with a one-hour special, followed by spinoff, star John Stamos says. He announced the show Monday night on "Jimmy Kimmel Live"
	Produced Summary: John Stamos announces Monday night that a Netflix spinoff is scheduled. The special will feature Candace Cameron Bure, who played Stephanie Tanner in the original series. The new series will feature Andrea Barber, who portrayed D.J.'s best friend Kimmy Gibbler.
8:
	Original Summary: Reanne Evans faced Ken Doherty in World Championship qualifier. Doherty won the world championship in 1997. Evans lost the first frame 71-15 against Doherty. But the Dudley native fought back to lead 4-3. Ken Doherty, however, managed to close out an enthralling contest 10-8.
	Produced Summary: Reanne Evans, 29, won the world championship with 127 male players in April. She was a winner at Sheffield's Crucible Theatre on Thursday night. Evans's match will be the first to be played in the world snooker championship.
9:
	Original Summary: Gang have been jailed for a total of 31 years for sexually abusing children. Offences happened in cars, woods or at the defendants' homes in Banbury. Lured victims to parties organised on social media and then abused them. Girls aged between 13 and 16 were exploited by the gang from 2009 to 2014.
	Produced Summary: Six men jailed for a total of 31 years after being convicted of a string of sexual offences. From 2009 to 2014, the offences ranged from inciting sexual activity with a child to rape. Victims described the parties as 'a place where girls would choose their targets' They were lured to parties organised by Ahmed Hassan-Sule, 21, known as 'Fiddy' One child described the parties as 'emotionally immature'

Process finished with exit code 0
